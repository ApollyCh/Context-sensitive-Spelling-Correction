{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Context-sensitive Spelling Correction",
   "id": "b4e572c38a03caf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:25:02.757737Z",
     "start_time": "2025-02-25T10:25:02.215030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import brown, reuters, webtext\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.util import bigrams, trigrams\n",
    "from functools import lru_cache\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ],
   "id": "8f2a961d4319e418",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Norvig's Solution\n",
    "Firstly, I decided to implement Norvig's solution as a baseline for future comparison. To ensure a fair comparison, I will use his original dataset rather than an expanded version. This allows me to measure my future improvements objectively when incorporating additional techniques later."
   ],
   "id": "1a2c502dde01460c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:25:03.034063Z",
     "start_time": "2025-02-25T10:25:02.760251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "\n",
    "WORDS = Counter(words(open('data/big.txt').read()))\n",
    "\n",
    "\n",
    "def P(word, N=sum(WORDS.values())):\n",
    "    return WORDS[word] / N\n",
    "\n",
    "\n",
    "def correction(word):\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "\n",
    "def candidates(word):\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "\n",
    "def known(words):\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "def edits2(word):\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ],
   "id": "dfa4647ab63ce556",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:25:03.037554Z",
     "start_time": "2025-02-25T10:25:03.034433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def norvig_sentence_correction(sentence):\n",
    "    if not sentence.strip():\n",
    "        return sentence\n",
    "\n",
    "    words = word_tokenize(sentence)\n",
    "    corrected_words = [correction(word) for word in words]\n",
    "\n",
    "    return \" \".join(corrected_words)"
   ],
   "id": "ac5a6448aa29884a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation\n",
    "To evaluate the spelling correction model, I will use the following metric:\n",
    "- Word F1-score\n",
    "- Character Accuracy\n",
    "- Word Error Rate (WER)"
   ],
   "id": "5ece79fdc6df85f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:25:03.063582Z",
     "start_time": "2025-02-25T10:25:03.037554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "\n",
    "\n",
    "def word_f1(true_words, corrected_words):\n",
    "    min_length = min(len(true_words), len(corrected_words))\n",
    "    true_words = true_words[:min_length]\n",
    "    corrected_words = corrected_words[:min_length]\n",
    "\n",
    "    return f1_score(true_words, corrected_words, average='micro')\n",
    "\n",
    "\n",
    "def word_error_rate(true_sentence, corrected_sentence):\n",
    "    true_words = word_tokenize(true_sentence)\n",
    "    return Levenshtein.distance(true_sentence, corrected_sentence) / len(true_words)\n",
    "\n",
    "\n",
    "def evaluate_spelling_correction(true_sentences, corrected_sentences):\n",
    "    word_f1_scores = []\n",
    "    char_accuracies = []\n",
    "    wer_scores = []\n",
    "\n",
    "    for true_sentence, corrected_sentence in zip(true_sentences, corrected_sentences):\n",
    "        true_words = word_tokenize(true_sentence)\n",
    "        corrected_words = word_tokenize(corrected_sentence)\n",
    "\n",
    "        word_f1_scores.append(word_f1(true_words, corrected_words))\n",
    "\n",
    "        # Ensure correct handling when sentences differ in length\n",
    "        char_acc = np.mean([1 if c1 == c2 else 0 for c1, c2 in zip(true_sentence, corrected_sentence)]) \\\n",
    "            if len(true_sentence) == len(corrected_sentence) else 0\n",
    "        char_accuracies.append(char_acc)\n",
    "\n",
    "        wer_scores.append(word_error_rate(true_sentence, corrected_sentence))\n",
    "\n",
    "    print(f\"Spelling Correction Evaluation Results:\")\n",
    "    print(f\"Word F1-score: {np.mean(word_f1_scores):.2f} (Higher is better)\")\n",
    "    print(f\"Character Accuracy: {np.mean(char_accuracies):.2f} (Higher is better)\")\n",
    "    print(f\"Word Error Rate (WER): {np.mean(wer_scores):.2f} (Lower is better)\")"
   ],
   "id": "4319e599773328d6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For the evaluation part I find a dataset on Hugging Face that include only two types of sentences: \n",
    "- *Target*: sentences without spelling errors.\n",
    "- *Source*: sentences with spelling mistakes"
   ],
   "id": "d4df1a74b93fdc42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:25:07.449250Z",
     "start_time": "2025-02-25T10:25:03.064596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"vishnun/SpellGram\")['train']\n",
    "ds"
   ],
   "id": "8fa3829ff6d0cc7c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target'],\n",
       "    num_rows: 40000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test dataset is large enough, so I will use only part of it",
   "id": "87c3473b6e75fa96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:25:07.499775Z",
     "start_time": "2025-02-25T10:25:07.449761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_sentences = ds['target'][1000:7000]\n",
    "corrected_sentences = ds['source'][1000:7000]"
   ],
   "id": "cfc670d5007af481",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:02.858447Z",
     "start_time": "2025-02-25T10:25:07.499775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "improved_sentences = []\n",
    "\n",
    "for sentence in tqdm(corrected_sentences):\n",
    "    improved_sentences.append(norvig_sentence_correction(sentence))\n",
    "\n",
    "evaluate_spelling_correction(true_sentences, improved_sentences)"
   ],
   "id": "80058cceeac444a7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [03:51<00:00, 25.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling Correction Evaluation Results:\n",
      "Word F1-score: 0.85 (Higher is better)\n",
      "Character Accuracy: 0.63 (Higher is better)\n",
      "Word Error Rate (WER): 0.21 (Lower is better)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## My implementation\n",
    "### Data preprocessing\n",
    "As the first improvement, I will use a dataset from the NLTK library. This dataset contains a large vocabulary, which will help improve spelling correction by providing a more comprehensive reference for word probabilities and possible corrections. A larger corpus increases the chances of correctly identifying and ranking candidate words, leading to better overall accuracy."
   ],
   "id": "c146ec7be9dc5e3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:13.178643Z",
     "start_time": "2025-02-25T10:29:02.859455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download(\"brown\")\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"webtext\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "texts = brown.sents() + reuters.sents() + webtext.sents()\n",
    "\n",
    "tokens = []\n",
    "for sent in texts:\n",
    "    clean_sent = [word.lower() for word in sent if word.isalnum()]\n",
    "    if clean_sent:\n",
    "        tokens.extend([\"<s>\"] + clean_sent + [\"</s>\"])\n",
    "\n",
    "unigram_freq = Counter(tokens)"
   ],
   "id": "52093bc90ee43078",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:14.938774Z",
     "start_time": "2025-02-25T10:29:13.179517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bigram_list = [bg for bg in bigrams(tokens) if \"<s>\" not in bg and \"</s>\" not in bg]\n",
    "trigram_list = [tg for tg in trigrams(tokens) if \"<s>\" not in tg and \"</s>\" not in tg]\n",
    "\n",
    "bigram_freq = Counter(bigram_list)\n",
    "trigram_freq = Counter(trigram_list)"
   ],
   "id": "9057cb25bbba93ad",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:14.943860Z",
     "start_time": "2025-02-25T10:29:14.940292Z"
    }
   },
   "cell_type": "code",
   "source": "len(unigram_freq), len(bigram_freq), len(trigram_freq)",
   "id": "58ecd8f251f48bec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63598, 803180, 1679220)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Keyboard mapping\n",
    " I implemented keyboard mapping, which takes into account common typing errors caused by key proximity. This approach assigns higher probabilities to mistakes that are likely due to adjacent keys on a standard QWERTY keyboard. For example, mistyping \"g\" instead of \"f\" is more likely than replacing it with \"z.\""
   ],
   "id": "af919d5db5227f89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:14.954344Z",
     "start_time": "2025-02-25T10:29:14.944866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "keyboard = {\n",
    "    'a': 'qwsz',\n",
    "    'b': 'vghn',\n",
    "    'c': 'xdfv',\n",
    "    'd': 'erfcxs',\n",
    "    'e': 'rdsw',\n",
    "    'f': 'rtgvcd',\n",
    "    'g': 'tyhbvf',\n",
    "    'h': 'yujnbg',\n",
    "    'i': 'ujko',\n",
    "    'j': 'uikmnh',\n",
    "    'k': 'iolmj',\n",
    "    'l': 'opk',\n",
    "    'm': 'njk',\n",
    "    'n': 'bhjm',\n",
    "    'o': 'iklp',\n",
    "    'p': 'ol',\n",
    "    'q': 'wa',\n",
    "    'r': 'etdf',\n",
    "    's': 'wedxza',\n",
    "    't': 'rygf',\n",
    "    'u': 'yihj',\n",
    "    'v': 'cfgb',\n",
    "    'w': 'qesa',\n",
    "    'x': 'zsdc',\n",
    "    'y': 'tghu',\n",
    "    'z': 'asx'\n",
    "}"
   ],
   "id": "5a258f55161a93a6",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:14.959330Z",
     "start_time": "2025-02-25T10:29:14.955349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def keyboard_penalty(correct, wrong):\n",
    "    if correct == wrong:\n",
    "        return 1\n",
    "    elif wrong in keyboard.get(correct, \"\"):\n",
    "        return 0.95\n",
    "    else:\n",
    "        return 0.7"
   ],
   "id": "3f7b393e033a07d1",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Correction algorithm",
   "id": "496dd85724f5157d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:15.002707Z",
     "start_time": "2025-02-25T10:29:14.960335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import heapq\n",
    "\n",
    "\n",
    "def generate_keyboard_edits(word):\n",
    "    edits = set()\n",
    "    word = word.lower()\n",
    "\n",
    "    for i, char in enumerate(word):\n",
    "        if char in keyboard:\n",
    "            for neighbor in keyboard[char]:\n",
    "                typo = word[:i] + neighbor + word[i + 1:]\n",
    "                edits.add((typo, 0.95))\n",
    "\n",
    "    return edits\n",
    "\n",
    "\n",
    "def generate_candidates(word):\n",
    "    letters = string.ascii_lowercase\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "\n",
    "    deletes = {(L + R[1:], 0.2) for L, R in splits if R}\n",
    "    transposes = {(L + R[1] + R[0] + R[2:], 0.5) for L, R in splits if len(R) > 1}\n",
    "    inserts = {(L + c + R, 0.3) for L, R in splits for c in letters}\n",
    "\n",
    "    replaces = {\n",
    "        (L + c + R[1:], keyboard_penalty(R[0], c))\n",
    "        for L, R in splits if R for c in letters\n",
    "    }\n",
    "\n",
    "    keyboard_replaces = generate_keyboard_edits(word)\n",
    "\n",
    "    return deletes | transposes | replaces | inserts | keyboard_replaces\n",
    "\n",
    "\n",
    "def generate_edit2_candidates(word, k=10):\n",
    "    edit1 = generate_candidates(word)\n",
    "    edit2 = {candidate for candidate, _ in edit1 if candidate not in unigram_freq}\n",
    "\n",
    "    return set(heapq.nlargest(k, {cand: unigram_freq.get(cand, 0) for cand in edit2}.items(), key=lambda x: x[1]))\n",
    "\n"
   ],
   "id": "398f69689f4c8f9c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:15.008349Z",
     "start_time": "2025-02-25T10:29:15.003971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@lru_cache(maxsize=100000)\n",
    "def unigramProbability(word):\n",
    "    total = sum(unigram_freq.values())\n",
    "    return unigram_freq.get(word, 0) / total if total > 0 else 0"
   ],
   "id": "ad10ebe446cd8b6",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:15.012577Z",
     "start_time": "2025-02-25T10:29:15.008349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@lru_cache(maxsize=100000)\n",
    "def bigramProbability(word1, word2):\n",
    "    if not word1:\n",
    "        return unigramProbability(word2)\n",
    "\n",
    "    return bigram_freq.get((word1, word2), 0) / unigram_freq.get(word1, 1)"
   ],
   "id": "cfb46aaddccf84c8",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:15.017610Z",
     "start_time": "2025-02-25T10:29:15.013582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@lru_cache(maxsize=100000)\n",
    "def trigramProbability(word1, word2, word3):\n",
    "    if not word1:\n",
    "        return bigramProbability(word2, word3)\n",
    "    if not word2:\n",
    "        return unigramProbability(word3)\n",
    "    return trigram_freq.get((word1, word2, word3), 0) / bigram_freq.get((word1, word2), 1)"
   ],
   "id": "8a2ed8051cc8ac2a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Spelling",
   "id": "305d0f69a3783546"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:15.023200Z",
     "start_time": "2025-02-25T10:29:15.018969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def correct_spelling(word, previous_word, previous_prev_word, trigram=False):\n",
    "    if word in unigram_freq:\n",
    "        return word\n",
    "\n",
    "    candidates = generate_candidates(word)\n",
    "    known_candidates = {(w, _) for w, _ in candidates if w in unigram_freq}\n",
    "\n",
    "    if not known_candidates:\n",
    "        edit2_candidates = generate_edit2_candidates(word)\n",
    "        known_candidates = {(w, _) for w, _ in edit2_candidates if w in unigram_freq}\n",
    "\n",
    "    if not known_candidates:\n",
    "        return word\n",
    "\n",
    "    if trigram:\n",
    "\n",
    "        best_candidate = max(known_candidates,\n",
    "                             key=lambda x: math.log(unigramProbability(x[0]) + 1e-10) +\n",
    "                                           2 * math.log(\n",
    "                                 bigramProbability(previous_word, x[0]) + 1e-10) +\n",
    "                                           3 * math.log(trigramProbability(previous_prev_word, previous_word,\n",
    "                                                                           x[\n",
    "                                                                               0]) + 1e-10) +\n",
    "                                           math.log(x[1] + 1e-10) +\n",
    "                                           math.log(unigram_freq.get(x[0], 1e-10)))\n",
    "\n",
    "    else:\n",
    "        best_candidate = max(known_candidates,\n",
    "                             key=lambda x: math.log(unigramProbability(x[0]) + 1e-10) +\n",
    "                                           2 * math.log(\n",
    "                                 bigramProbability(previous_word, x[0]) + 1e-10) +\n",
    "                                           math.log(x[1] + 1e-10) +\n",
    "                                           math.log(unigram_freq.get(x[0], 1e-10)))\n",
    "\n",
    "    return best_candidate[0]\n"
   ],
   "id": "bf99601af484dca4",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:15.029443Z",
     "start_time": "2025-02-25T10:29:15.023200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def correct_sentence(sentence, trigram=False):\n",
    "    if not sentence.strip():\n",
    "        return sentence\n",
    "\n",
    "    words = word_tokenize(sentence)\n",
    "    corrected_words = []\n",
    "    prev_word, prev_prev_word = \"\", \"\"\n",
    "\n",
    "    for word in words:\n",
    "        corrected_word = correct_spelling(word, prev_word, prev_prev_word, trigram=trigram)\n",
    "        corrected_words.append(corrected_word)\n",
    "        prev_prev_word, prev_word = prev_word, corrected_word\n",
    "\n",
    "    return \" \".join(corrected_words)"
   ],
   "id": "bf7ec7523f788e26",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:15.035451Z",
     "start_time": "2025-02-25T10:29:15.030448Z"
    }
   },
   "cell_type": "code",
   "source": "correct_sentence(\"i study at the universoty\")",
   "id": "d24d87a5d0642c48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i study at the university'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation",
   "id": "23c5867815350c12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:25.198160Z",
     "start_time": "2025-02-25T10:29:15.035959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "improved_sentences = []\n",
    "\n",
    "for sentence in tqdm(corrected_sentences):\n",
    "    improved_sentences.append(correct_sentence(sentence))\n",
    "\n",
    "evaluate_spelling_correction(true_sentences, improved_sentences)"
   ],
   "id": "84c5bad1e78146e9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:05<00:00, 1035.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling Correction Evaluation Results:\n",
      "Word F1-score: 0.86 (Higher is better)\n",
      "Character Accuracy: 0.82 (Higher is better)\n",
      "Word Error Rate (WER): 0.19 (Lower is better)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:29:32.496417Z",
     "start_time": "2025-02-25T10:29:25.199168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "improved_sentences = []\n",
    "\n",
    "for sentence in tqdm(corrected_sentences):\n",
    "    improved_sentences.append(correct_sentence(sentence, trigram=True))\n",
    "\n",
    "evaluate_spelling_correction(true_sentences, improved_sentences)"
   ],
   "id": "10bb3fffa2f76951",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:02<00:00, 2061.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling Correction Evaluation Results:\n",
      "Word F1-score: 0.86 (Higher is better)\n",
      "Character Accuracy: 0.82 (Higher is better)\n",
      "Word Error Rate (WER): 0.19 (Lower is better)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Justification\n",
    "## N-grams\n",
    "The primary limitation in Nodvig's solution was the use of a small dataset. To address this, I leveraged much larger datasets from the NLTK library, which provided a more robust base for calculating word probabilities and correcting spelling errors. Additionally, Nodvig's method relied on unigrams, which is less effective for context-dependent words that are related to others in the sentence. \n",
    "\n",
    "\n",
    "Additionally, I integrated trigrams into the solution, though I found that they can slow down the algorithm. The impact of trigrams, while noticeable, is not always critical, as the evaluation results with and without trigrams were very similar. Therefore, I left the use of trigrams as an optional feature. If accuracy is the priority, trigrams are valuable, but they can be omitted for speedier performance.\n",
    "\n",
    "## Keyboard Map and Weights\n",
    "One important improvement I made was the inclusion of weighted errors. Many common spelling mistakes are caused by the keyboard layout, and so I assigned different weights to various types of errors based on their likelihood of occurrence. These weights were carefully chosen and tested, and here’s how they work:\n",
    "- Keyboard mistakes (keyboard_replaces): 0.95 (very common errors, such as mistyped adjacent keys)\n",
    "- Replacements (replaces): 0.8 (frequent but less common than keyboard-related mistakes)\n",
    "- Transpositions (transposes): 0.5 (less frequent, such as switching adjacent letters)\n",
    "- Insertions (inserts): 0.3 (rare, when extra characters are typed)\n",
    "- Deletions (deletes): 0.2 (also rare, where characters are omitted)\n",
    "\n",
    "This weighted approach allows for better prioritization during the search for potential corrections. It ensures that more frequent errors, such as those arising from adjacent key presses, are given higher priority.\n",
    "\n",
    "## Improve Speed\n",
    "Another notable improvement I made was to optimize the edit2 generation process using heapq. Previously, generating all possible candidate edits for the second iteration (edit2) could be computationally expensive. By using a heap, I ensured that only the best candidates were prioritized, and I reduced the impact of low-weighted corrections. \n",
    "\n",
    "Also, I implemented caching to speed up the code. All computations are now much faster, as results of frequently repeated operations (such as unigram, bigram, and trigram probabilities) are cached. This avoids redundant calculations and significantly improves performance.\n",
    "\n",
    "## Best Candidate Function\n",
    "When selecting the best candidates, I implemented a maximization function, which can be quite large and complex due to the many variables at play. To avoid the issue of probabilities becoming excessively small, I employed logarithms. This ensures that the values stay manageable and helps stabilize the calculation process. Additionally, I adjusted the weights for bigrams and trigrams to give more importance to these models, recognizing their higher contextual value.\n",
    "\n",
    "## Evaluation Results\n",
    "The difference between the results is not extremely significant, but my implementation shows much better results for Character Accuracy. I believe this improvement is largely due to the incorporation of bigram and trigram models, which capture more context and help correct mistakes at the character level. Additionally, the speed improvement is quite noticeable, thanks to optimizations like caching and more efficient handling of edit candidates. These changes contribute to both better performance and faster processing.\n"
   ],
   "id": "3b9a026f03a33a7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
